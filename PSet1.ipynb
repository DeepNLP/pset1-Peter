{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224D: Assignment #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a - Prove that the softmax is invariant to constant offsets in the input, that is, for any input vector $x$ and any constant $c$, \n",
    "$softmax(x) = softmax(x + c)$\n",
    "when $x + c$ means adding the constant $c$ to every dimension of $x$. Remember that: \n",
    "\n",
    "$softmax(x)_i = \\frac{e^{x_i}}{\\sum_je^{x_j}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a - anwer: Adding a constant $c$ changes both the numerator and the denominator the same way, which does not affect the result after division. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually computationnaly difficult to compute sums of exponentials with low arguments. Hence, we will use this property to normalize the arguments  and set $c = -max_i x_i$, ie that we substract the maximum element from all elements of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b - Given an input matrix of N rows and d columns, compute the softmax predictions for each row. Write your implementation in q1_softmax.py. You may test by executing python q1_softmax.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax function for each row of the input x.\n",
    "\n",
    "    It is crucial that this function is optimized for speed because\n",
    "    it will be used frequently in later code.\n",
    "    You might find numpy functions np.exp, np.sum, np.reshape,\n",
    "    np.max, and numpy broadcasting useful for this task. (numpy\n",
    "    broadcasting documentation:\n",
    "    http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
    "\n",
    "    You should also make sure that your code works for one\n",
    "    dimensional inputs (treat the vector as a row), you might find\n",
    "    it helpful for your later problems.\n",
    "\n",
    "    You must implement the optimization in problem 1(a) of the \n",
    "    written assignment!\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Moreover, to be as general as possible, we suppose that x is a 2D array, \n",
    "    N rows and d columns\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Remark: we must also consider the specific case where N = 1\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # Case N > 1\n",
    "    if len(x.shape) > 1:\n",
    "        \n",
    "        ## normalization part\n",
    "\n",
    "        # First define the column containing the max of each row in x\n",
    "        max_row = x.max(axis = 1).reshape((x.shape[0], 1))\n",
    "        # Then remove this max to the dataset\n",
    "        x -= max_row\n",
    "\n",
    "        ## computation of softmax\n",
    "\n",
    "        x = np.exp(x)\n",
    "        denominator = x.sum(axis = 1).reshape(x.shape[0], 1)\n",
    "        x /= denominator\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        ## normalization part\n",
    "        \n",
    "        max_row = x.max()\n",
    "        x -= max_row\n",
    "        x = np.exp(x)\n",
    "        denominator = x.sum()\n",
    "        x /= denominator\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26894142,  0.73105858])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Toy example\n",
    "softmax(np.array([1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test this functiondef test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started. \n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print \"Running basic tests...\"\n",
    "    test1 = softmax(np.array([1,2]))\n",
    "    print test1\n",
    "    assert np.amax(np.fabs(test1 - np.array(\n",
    "        [0.26894142,  0.73105858]))) <= 1e-6\n",
    "\n",
    "    test2 = softmax(np.array([[1001,1002],[3,4]]))\n",
    "    print test2\n",
    "    assert np.amax(np.fabs(test2 - np.array(\n",
    "        [[0.26894142, 0.73105858], [0.26894142, 0.73105858]]))) <= 1e-6\n",
    "\n",
    "    test3 = softmax(np.array([[-1001,-1002]]))\n",
    "    print test3\n",
    "    assert np.amax(np.fabs(test3 - np.array(\n",
    "        [0.73105858, 0.26894142]))) <= 1e-6\n",
    "\n",
    "    print \"You should verify these results!\\n\"def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started. \n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print \"Running basic tests...\"\n",
    "    test1 = softmax(np.array([1,2]))\n",
    "    print test1\n",
    "    assert np.amax(np.fabs(test1 - np.array(\n",
    "        [0.26894142,  0.73105858]))) <= 1e-6\n",
    "\n",
    "    test2 = softmax(np.array([[1001,1002],[3,4]]))\n",
    "    print test2\n",
    "    assert np.amax(np.fabs(test2 - np.array(\n",
    "        [[0.26894142, 0.73105858], [0.26894142, 0.73105858]]))) <= 1e-6\n",
    "\n",
    "    test3 = softmax(np.array([[-1001,-1002]]))\n",
    "    print test3\n",
    "    assert np.amax(np.fabs(test3 - np.array(\n",
    "        [0.73105858, 0.26894142]))) <= 1e-6\n",
    "\n",
    "    print \"You should verify these results!\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[ 0.26894142  0.73105858]\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "[[ 0.73105858  0.26894142]]\n",
      "You should verify these results!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started. \n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print(\"Running basic tests...\")\n",
    "    test1 = softmax(np.array([1,2]))\n",
    "    print(test1)\n",
    "    assert np.amax(np.fabs(test1 - np.array(\n",
    "        [0.26894142,  0.73105858]))) <= 1e-6\n",
    "\n",
    "    test2 = softmax(np.array([[1001,1002],[3,4]]))\n",
    "    print(test2)\n",
    "    assert np.amax(np.fabs(test2 - np.array(\n",
    "        [[0.26894142, 0.73105858], [0.26894142, 0.73105858]]))) <= 1e-6\n",
    "\n",
    "    test3 = softmax(np.array([[-1001,-1002]]))\n",
    "    print(test3)\n",
    "    assert np.amax(np.fabs(test3 - np.array(\n",
    "        [0.73105858, 0.26894142]))) <= 1e-6\n",
    "\n",
    "    print(\"You should verify these results!\\n\")\n",
    "test_softmax_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
